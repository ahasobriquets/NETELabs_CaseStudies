% Template for PLoS
% Version 3.3 June 2016
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % %

% Created to hold supplementary information

\documentclass[10pt,letterpaper]{article}

\usepackage[top=0.85in,left=2in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% added by George
%\usepackage{wrapfig}


% create \thickline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}

% WATERMARKS
%\usepackage{draftwatermark}
%\SetWatermarkText{Confidential}
%\SetWatermarkScale{4}

% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{27.023pt}
\lhead{\includegraphics[width=3.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{1.25in}
\fancyfootoffset[L]{1.25in}
\lfoot{\sf PLOS}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Research Synergy and Drug Development: Bright Stars in Neighboring Constellations} % Please use "title case" (capitalize all terms in the title except conjunctions, prepositions, and articles).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Samet Keserci\textsuperscript{1},
%Samet Keserci\textsuperscript{1,2\Yinyang},
Eric Livingston\textsuperscript{2},
Lingtian Wan\textsuperscript{1,\textcurrency},
Alexander R. Pico\textsuperscript{3},
%Name5 Surname\textsuperscript{2\ddag},
%Name6 Surname\textsuperscript{2\ddag},
 \& George Chacko\textsuperscript{1*}
%with the Lorem Ipsum Consortium\textsuperscript{\textpilcrow}
\\
\bigskip
\textbf{1} NETE Labs, NET ESolutions Corporation, McLean, VA, USA
\\
\textbf{2} Research Intelligence, Elsevier Inc., New York, NY, USA
\\
\textbf{3} Gladstone Institutes, San Francisco, CA, USA
\\
\bigskip


% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
%\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
%\ddag These authors also contributed equally to this work.

% Current address notes
\textcurrency Current Address: Facebook Inc., Menlo Park, CA, USA % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
%\dag Deceased

% Group/Consortium Author Note
%\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
*Correspondence: netelabs@nete.com
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}

Drug discovery and subsequent availability of a new breakthrough therapeutic or `cure' is a compelling example of societal benefit from research advances. These advances are invariably collaborative, involving the contributions of many scientists to a discovery network in which theory and experiment are built upon. To understand such scientific advances, data mining of public and commercial data sources coupled with network analysis can be used as a digital methodology to assemble and analyze component events in the history of a therapeutic. This methodology is extensible beyond the history of therapeutics and its use more generally supports (i) efficiency in  exploring  the scientific history of a research advance (ii) documenting and understanding collaboration (iii) portfolio analysis, planning and optimization (iv) communication of the societal value of research.  As a proof of principle, we have conducted a case study of five anti-cancer therapeutics. We have linked the work of over 235,000 authors in roughly 103,000 scientific publications that capture the research crucial for the development of these five therapeutics. We have enriched the content of networks of these therapeutics by annotating them with information on research awards as well as peer review that preceded these awards. Applying retrospective citation discovery, we have identified a core set of publications cited in the networks of all five therapeutics and additional intersections in combinations of networks as well as awards from the National Institutes of Health that supported this research. Lastly, we have mapped these awards to their cognate peer review panels, identifying another layer of collaborative scientific activity that influenced the research represented in these networks. 

\linenumbers

\section*{Introduction}

Data mining of public data sources coupled with network analysis enables the quantitative description of research discoveries that were influential in the development of a breakthrough therapeutic or `cure'.  The set of scientific publications, clinical trials, patents, and regulatory approvals, linked to each other by citation or assignment, that documents the progress of concepts from basic research to a cure is termed a `cure network' \cite {bibWilliams}. Key assumptions in constructing these networks are that the references found in relevant documents are appropriate citations of new knowledge relevant to a given cure and that a further retrospective round of citation discovery will reveal previous influential discovery (\textit{ibid}). Beyond using a digital methodology to assemble a set of facts about a major scientific advance, a deeper contextual understanding of knowledge diffusion across disciplines, scientific interests, culture, and time~\cite{bibMaldame} is enabled. Such studies (i) provide evidence for the broad collaborative platform of basic and translational research underlying major scientific advances such as cures for diseases \cite {bibLauer} (ii) support strategic communications to oversight bodies, and (iii) help communicate the societal value of research. Importantly too, when coupled with information that accrues from clinical use of a drug, knowledge of its development network enables a recursive uncovering of the pathogenesis of the disease it is being used to treat, as noted for the burgeoning field of immunotherapeutics~\cite{bibChan}. 

Williams and colleagues have elegantly demonstrated the feasibility and value of data mining and network analysis using, as case studies, ivacaftor and ipilimumab, approved for the treatment of cystic fibrosis and melanoma respectively (\textit{vide supra}). They observed that `the nature of a cure discovery network is complex and fundamentally collaborative', noting in the case of ivacaftor, that at least 7,067 scientists with 5,666 unique affiliations contributed to ivacaftor-relevant research over a period greater than 100 years. These authors also suggest that thoughtful metrics derived from this concept could inform decision making by funders.

Extending the methodology to study additional cures and significant research advances is a logical next step. Ascertaining the nature of the interactions, if any, between networks, is also of considerable interest since it supports an understanding of collaboration across networks as well as common features of science networks. Lastly, even if ambitious, scaling from case studies to mapping the entire domain of drug development is likely to be beneficial in planning, resource allocation and optimization of drug development activities. 

To address these questions, we have built upon prior art for single networks to (i) incorporate enhanced data mining methods and network metrics (ii) include enriched data from a commercially available bibliographic database with disambiguated author identifiers (iii) include information on research awards and peer review of grants (iv) extended single network analysis to map publications and authors across multiple networks. For evaluation, we have conducted case studies of a cluster of five FDA-approved therapeutics for cancer. We present the results of this case study as a body of work for further study by other researchers, and a step towards mapping the universe of FDA-approved drugs and biologicals.

\section*{Materials and Methods}  A set of five anti-cancer therapeutics, three drugs and two biologicals, approved for use in humans by the Food and Drug Administration was selected for this study (Table 1). Imatinib and Sunitinib are tyrosine kinase inhibitors, Nelarabine is a nucleoside analog, and Ramucirumab and Alemtuzumab (Campath) are humanized antibodies that target the CD52 and vEGFR-2 cell surface receptors respectively. For each of these therapeutics, a set of relevant scientific publications was constructed as in Williams et al.~\cite{bibWilliams} but with specific modifications detailed below. An allowance of 2 months was also made for `publication lag' when assembling referenced material. For example, if a therapeutic was approved on Jan 1, 2017, documents published  on or before March 31, 2017 were included. For each of the five therapeutics, a first-generation list of PubMed identifiers (citing\_pmid) was harvested from the five different data sources (below). \textbf{[Insert Table 1]}

\textbf{Clinical trials} The national clinical trials database (clinicaltrials.gov) was searched for clinical trials of the five therapeutics that completed by the data of FDA approval. Both cited references and publications from these clinical trials were collected if they were published within the approval date plus two months. To capture publications associated with the clinical trials that were not displayed in clinical trials.gov, PubMed was also searched with the unique identifier (NCT number) of any clinical trials that were identified. To capture publications of clinical trials not registered in clinicaltrials.gov, PubMed was searched using the therapeutic name as keyword, publication type as ``clinical trial'', and an appropriate date restriction as in searches of clinicaltrials.gov. For example, the search term (((``alemtuzumab"[Supplementary Concept] OR ``alemtuzumab"[All Fields]) OR (``alemtuzumab"[Supplementary Concept] OR ``alemtuzumab"[All Fields] OR ``campath"[All Fields])) AND (``1900/0101"[PDAT] : ``2001/07/31''[PDAT])) AND ``clinical trial"[Publication Type] was used to identify publications of clinical trials for Alemtuzumab.

\textbf{FDA documents} The drugs@fda website~\cite{bibFDA} was searched for each of the five therapeutics and cited references in the medical review document were manually extracted and matched to pmids. FDA Approval Summaries published in journals by FDA staff, were available for all five therapeutics and contain cited references. If the published date of a cited reference in an Approval Summary exceeded the approval date plus two months, the publication was not included.

\textbf{Patents} For each therapeutic a single patent was identified that best represented the most relevant invention to the therapeutic at hand. Identification of this patent was performed using multiple web sources. The US patent number was then used to identify the patent and the non-patent citation list from Google Patents ~\cite{bibGooglePatents} was manually processed by searching PubMed for appropriate pmids. The accuracy of manual searches was far higher than a citation matching tool that we developed for for this purpose,and were used to generate the data in this study.

\textbf{Post-approval literature reviews} Review articles published after a therapeutic's approval by the FDA are independent studies of the the development of a therapeutic. Accordingly, PubMed was searched for review articles on these five therapeutics that were published between the date of FDA approval and a year following the date of approval. Cited references in these reviews were extracted using PubMed and Scopus. The review articles themselves were not included. 

\textbf{Pre-approval literature searches} Literature searches were performed using PubMed with a date range of 1900/01/01 to two months post-FDA approval. For example, the search term ((alemtuzumab) OR campath) AND (``1900/01/01"[Date - Publication] : ``2001/07/31"[Date - Publication]) was used to retrieve articles of interest relevant to alemtuzumab.

\textbf{PubMed and Scopus} Citing\_pmids from the five different sources above  were combined and deduplicated. Using the Scopus database and its APIs. The manually-generated list of pmids taken from the five sources mentioned (Clinical Trials, FDA Documents, Patents, Post-Approval Literature, and Pre-Approval Literature Searches) were searched in Scopus, using the basic Scopus Search API, to arrive at a list of Scopus IDs (citing\_sid) for the publications. The Scopus Abstract Retrieval API was then used to retrieve a more comprehensive record for each of the SIDs comprising that list of publications Next, for each of these publication records citing\_sid), we used the Scopus Author Retrieval API to retrieve a full record for each unique author in the  publication set. We also used the Abstract Retrieval API to collect records for each of the publications cited by the first generation of publications. This set of cited publications is the cited\_sid set.  Using the same Author Retrieval API, we then gathered data for each of the unique authors affiliated with the cited\_sid publications. Completion of the process yields two sets of publications, citing\_sid and cited\_sid, with citation links between them and full information on all authors for both generations. Finally, for each author in the study, we used the standard Scopus Search API once more to retrieve a smaller record for every publication affiliated with them in Scopus, in order to tally their overall publication output. While author records in Scopus have overall publication counts as part of the record, by manually downloading each of them, we can store and count them by type (i.e. article, book chapter, Editorial, review, etc.). This allowed us to more precisely arrive at publication totals for only those publication types that are relevant for this study. [which are]

Whereas mapping between PubMed and Scopus identifiers at the citing\_pmid and citing\_sid stage resulted in 1\% or less information loss, mapping at the cited\_sid to cited\_pmid resulted in a loss of roughly 15-20 \% of target records. Accordingly the Scopus data  was used as the backbone of the publication component of the network and the cited\_pmid information was treated as an annotation layer.  These observations are summarized in Table 2. \textbf{[Insert Table 2]}

Both citing and cited pmids were mapped to NIH grants and peer review panels (study sections) using public information available through NIHExPORTER~~\cite{bibNIHExPORTER}. Thus, we enriched our network data by identifying those study sections associated with the awards that supported publications in our networks. 

\textbf{Networks} The resultant data were modeled as networks and analyzed using metrics based on network topology. We calculated the propagated in-degree rank (PIR) and ratio of basic rankings (RBR) metrics of Williams ~~\cite{bibWilliams}. PIR represents the sum of weighted citation scores for all articles in a network that can be attributed to an author. In addition to computing PIR for all authors in each network, we also combined the citation data for all five networks and computed a networkPIR (nPIR) score, which was also normalized to the sum of individual PIR scores within each network as the PIRpartitionRatio (PPR) as a way to measure inter-network influence. RBR is intended to represent the fraction of a researcher's output that is in a network and is defined as the ratio of the number of publications in network to the number of publications in a background dataset for an author. In its original specification, the background dataset for RBR was constructed by keyword searches of PubMed. A potential weakness of this keyword based approach is that keywords do not effectively capture the field or the total output of an author even if multiple background samples are taken. Therefore, we created two new variants of the RBR; network RBR (nRBR) and global-based RBR (gRBR). nRBR uses all publications in our set of five therapeutics as background and dRBR takes advantage of the Scopus author\_id to capture the total article output of an author as background. Thus, nRBR and dRBR normalize a researcher's in-network contributions to backgrounds based on total network and total researcher productivity respectively. The details of how these metrics were calculated are provided in supplementary material \nameref{S1_File}.\\

\textbf{Analysis}. All data used in this study were acquired exclusively from the sources listed above. Computations were performed on infrastructure owned or leased by NET ESolutions, Elsevier , or the Gladstone Institutes. Code and scripts used in this study were written in Java, Python, SQL, and R and are archived on a publicly accessible Github repository~\cite{bibGithub}.  Network visualization was performed using Cytoscape~\cite{bibCytoscape}.
\section*{Results and Discussion} 

\textbf{Publications} Scientific publications form the backbone of each of these five networks. Our initial assumptions of appropriate citation and retrospective citation discovery (Introduction) suggest that network nodes that are common to multiple networks are likely to be influential. We calculated intersection counts for all possible combinations of publications in the Alemtuzumab, Imatinib, Nelarabine, Ramucirumab, and Sunitinib networks (\nameref{S2_Table 1}). We also applied intersection analysis at a finer level of granularity by computing intersection counts for both first generation citations (citing\_pmid) and second generation citations (cited\_sid). The results are displayed as Venn diagrams in Figure 1. \textbf{[Insert Fig. 1]}

The intersection of all five networks consists of 14 publications out of a total of 91,991 unique Scopus identifiers. Strikingly, not even a single publication is common to all five networks at the first generation level (citing\_sid) although a single publication, the pathbreaking work of Kohler and Milstein on the production of monoclonal antibodies~\cite{bibKohler}, is cited in four out of five networks. All 14 publications are in the second generation of citations (cited\_sid) and another 198 comprise the sum of intersections in all possible four-network combinations, roughly an order of magnitude greater than the case of cited references. We manually grouped these 14 publications using high level descriptive terms and observed that this group was composed of  statistical methods (5 publications), molecular and cell biological methods (4 publications), analytical and structural biology techniques (3 publications) and cancer biology (2 publications). Of these last two, one is a review of the p53 gene~\cite{bibLevine} and the second is a study of angiogenesis in children with acute lymphoblastic leukemia~\cite{bibFolkman}. Thus, the majority of this small set of 14 publications describes methods that are heavily cited in these therapeutic development networks, which is consistent with observations of the general scientific literature ~\cite{bibVanNoorden}. The relationship between core publications and their therapeutic networks is visualized in Fig 3. \textbf{[Insert Fig. 2]}. As the subject of another study, we are actively working on a scalable automated strategy to characterize the entire dataset as well as all combinations of intersections between networks using high level descriptive terms. 

\textbf{Grant Support} With its annual budget of approximately US\$32 billion, NIH is a major funder of biomedical research through its granting program. Understanding the nature and extent of NIH grant support for the research represented in our five networks, provides insight into the funding programs that enabled this research. We took advantage of publicly available data~\cite{bibNIHExPORTER} to identify grant support for the publications in our five networks by mapping them to pmids. A total of 19,104 unique grant numbers was harvested of which 112 were found in all five networks. At the intersection of five networks, the reason the number of grants is larger than the number of publications is because publications and grants exist in a `many to many' relationship in that each publication can acknowledge support from multiple grants and each grant can support multiple publications. These awards were grouped by major type  and visualized (Fig 3.). Of note, support from Research Program Projects and Center grants is proportionately larger in the intersection group when compared to the total population where the proportion of research projects is larger. A significant loss of information occurs when mapping from cited\_sid to cited\_pmid (Table 2). Thus we believe that these numbers are an underestimate of actual grant support from NIH. Also missing from this analysis, however, are details of research support from other funding agencies and industry, which are questions that we intend to pursue. Even so, these data testify to a recurring theme of collaboration and breadth of community engagement that is also seen at the publication level. We speculate that the broader and collaborative nature of such awards may be more likely to result in a methods-rich population of publications than the more focused research project award but elucidation will require further and more rigorous study. 

\textbf{Peer Review} Research support from NIH is typically made through a two stage peer-review process.  The Center for Scientific Review at NIH manages first-stage scientific review of between 50,000-60,000 grant applications each year~\cite{bibBoyack}, a process involving more than 15,000 expert reviewers. In addition, individual Institutes and Centers at NIH manage smaller scale peer review operations. Crudely estimating success rate at 20\%, peer review plays the role of a collaborative scientific activity and that serves as a selection layer strongly influencing granting outcomes. To describe this layer at a high-level, we matched the awards in the the five networks to the peer review panels (study sections) that evaluated them for scientific merit and calculated the intersection and union of these peer review panels. Eighty eight unique panel identifiers formed the intersection. Of these, 11 are distinguishable as Special Emphasis Panels that could be either one-time or recurring panels with temporary members, the remaining 77 are chartered panels with relatively stable membership. Some of these panels are no longer active and public records are not easily available to determine their scientific focus. For the 74 panels that could be classified (Supplemental), beyond an obvious focus on cancer, it is evident that the panels represent a rich mix of disciplines such as chemistry, biophysics, genetics, cell biology, and molecular biology; as well as AIDS, pathology, radiology, endocrinology, neurology, mental health, and child health. Four hundred and seven unique panel identifiers formed the union of all five networks. Of these 28 were Special Emphasis Panels, the remaining 379 panels were chartered as in the case of the intersection. These data provide evidence of broad input from invited experts in a collaborative activity that selects promising scientific projects. Assuming an average of 25 reviewers per panel (the number is likely to range from 5-40) and excluding that some of these panels are likely to have met multiple times during the lifespan of the awards in question and that some of these applications for funding may have been reviewed multiple times, a minimum of 10,000 experts comprised this additional layer of scientific influence. We believe that the actual number is likely to be at least double. A more accurate estimate would be possible if historical records of participation in peer review were made publicly available by NIH. We do not have records of awards or peer review from sources other than NIH and this also is a focus of future investigation.

\textbf{Network Metrics} To quantify network data and to identify influential researchers in and across networks, we calculated PIR and RBR scores for all researchers as well as nPIR, PPR, nRBR, and gRBR scores (Materials and Methods, Supplemental Information). The nPIR metric describes the sum of weighted citation scores for all articles that a researcher has in all five networks. When combined with the PIRpartitionRatio (PPR), insight into cross-network influence is generated.  A limitation of the nPIR and nRBR measurements is that they are valid only for the network(s) being studied. Scaling from five to the more than 1400 drugs approved by the FDA (and their many variants) would attenuate this limitation~\cite{bibKinch} but this goal is unlikely to be achieved in the very short term and other data quality issues are likely to emerge. 

While theoretically appealing (Materials and Methods), the gRBR is the most sensitive to data quality since it relies on an accurate estimate of total productivity of a researcher. We found several instances in the top 10\% of PIR scores where the gRBR was significantly inaccurate (data not shown). This metric is therefore likely to be useful when the author disambiguation problem and article capture is resolved to the point where data quality is significantly improved and is not recommended except when strong confidence exists in the total productivity counts. In general, these metrics may be best used in conjunction with positional measures such as quantiles to define populations of researchers within related networks, e.g, the top 25 researchers based on nPIR scores of all researchers in our dataset (\nameref{S3_Table 2}). These `bright stars in neighboring constellations', represent elite performers in network(s) of clinical and basic science expertise again reinforcing the concept of collaborative translational achievements built upon a body of basic science. Beyond simple weighting and normalization that we have used, a variety of citation metrics such as SNIP~\cite{bibWaltman}, with different normalization strategies at the field, journal, and article level are available for impact analysis and these could be applied to such networks depending on the features of these networks and the aim of the study~\cite{bibIoannidis}. 

In summary, we have developed and demonstrated a methodology not restricted to  drug discovery and cures alone, that offers burden-reduction in explorations of science history. It contributes to the understanding of collaboration across domains and can be used to  enrich portfolio analysis, planning and optimization, as well as communications of the societal value of research. The approach can be easily adapted to study the collaborative history within and across research portfolios of groups of researchers and targeted programs such as the Clinical and Translational Science Award (CTSA) Program. While finer critical evaluation of the content of datasets generated through this approach is best left to experts, the methodology is broadly accessible and can also be viewed as another tool for citizen science. Overall, no single metric will provide useful answers, instead expert interpretation of multiple metrics best matched to curated datasets will yield greater value.

\section*{Supporting Information}

\paragraph*{S1 File.}
\label{S1_File}
{\bf Network Calculations} The basis of calculations for network metrics.

\paragraph*{S2 Table 1}
\label{S2_Table 1}
{\bf Intersecting Publications Across Networks.} 

\paragraph*{S3 Table 2}
\label{S3_Table 2}
{\bf Elite Performers in Networks} 

\section*{Acknowledgments} We thank Sandeep Somaiya from NETE as well as Daniel Calto, M'hamed Aisati, Sherif El Shamy, and Holly J. Falk-Krzesinski from Elsevier for their support of this collaborative effort.

\nolinenumbers
 
\begin{thebibliography}{10}

\bibitem{bibWilliams}
Williams RS, Lotia S, Holloway AK, Pico AR.
\newblock {{F}rom Scientific Discovery to Cures: Bright Stars within a Galaxy.}
\newblock Cell. 2015 163:21--23.

\bibitem{bibMaldame}
Maldame, J.
\newblock {{T}he Importance Of The History Of Science In Intellectual Formation.}
\newblock Scripta Varia 2002 104:237-248.

\bibitem{bibLauer}
Lauer MS.
\newblock {{P}CSK9 Inhibitors: Lots of Work Done, Lots More to Do.}
\newblock Ann Intern Med. 2016 164(9):624-625.

\bibitem{bibChan}
O'Shea JJ, Kanno, Y., Chan AC.
\newblock {{I}n Search of Magic Bullets: The Golden Age of Immunotherapeutics.}
\newblock Cell. 2014 157:227-240.

\bibitem{bibFDA}
Federal Drug Administration.
\newblock {{D}rugs@FDA: FDA Approved Drug Products.}
\newblock https://www.accessdata.fda.gov/scripts/cder/daf/.

\bibitem{bibGooglePatents}
Google Inc.
\newblock {{G}oogle Patents}
\newblock https://patents.google.com/.

\bibitem{bibNIHExPORTER}
National Institutes of Health.
\newblock {{N}IH ExPORTER}
\newblock https://exporter.nih.gov/.

\bibitem{bibGithub}
\newblock {{N}ET ESolutions, NETE Labs}
\newblock https://github.com/NETESOLUTIONS/NETELabs\_CaseStudies/tree/master/Bright\_Stars.

\bibitem{bibCytoscape}
Shannon P, Markiel A, Ozier O, Baliga N, Wang JT, Ramage D, et al.
\newblock {{C}ytoscape: a software environment for integrated models of biomolecular interaction networks.}
\newblock Genome Res. 2003 13(11):2498-504.

\bibitem{bibKohler}
K\"ohler, G., Milstein, C.
\newblock { {C}ontinuous cultures of fused cells secreting antibody of predefined specificity.}
\newblock Nature. 1975 256: 495-497.

\bibitem{bibLevine}
Levine, AJ.
\newblock {{p53}, the cellular gatekeeper for growth and division.}
\newblock Cell. 1997 88(3):323-31.

\bibitem{bibFolkman}
Perez-Atayde AR, Sallan SE, Tedrow U, Connors S, Allred E, Folkman J.
\newblock {{Spectrum} of tumor angiogenesis in the bone marrow of children with acute lymphoblastic leukemia}
\newblock Am J Pathol. 1997 150(3):815-21.

\bibitem{bibVanNoorden}
Van Noorden R, Maher B, Nuzzo R.
\newblock {{T}he top 100 papers}
\newblock Nature. 2014 514: 550-553. 

\bibitem{bibBoyack}
Boyack KW, Chen M-C, Chacko G. 
\newblock {{C}haracterization of the Peer Review Network at the Center for Scientific Review, National Institutes of Health}
\newblock  PLoS One. 2014 10.1371/journal.pone.0104244.

\bibitem{bibKinch}
Kinch MS, Haynesworth A, Kinch SL, Hoyer D.
\newblock {{A}n overview of FDA-approved new molecular entities: 1827-2013}
\newblock  Drug Discov Today. 2014 19(8):1033-9 doi: 10.1016/j.drudis.2014.03.018.

\bibitem{bibWaltman}
Waltman L, van Eck NJ, van Leeuwen TN, Visser, MS.
\newblock {{S}ome modifications to the SNIP journal impact indicator.}
\newblock  J Informetrics. 2013. 7(2): 272-285.

\bibitem{bibIoannidis}
Ioannidis JPA, Boyack K, Wouters PF.
\newblock {{C}itation Metrics: A Primer on How (Not) to Normalize.}
\newblock  PLoS Biology. 2016. 10.1371/journal.pbio.1002542.

\end{thebibliography}
\end{document}

